{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPSy3VPHL5Uq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from ResNet import Bottleneck, ResNet, ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFFakdIfNEEZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "V2SCe0hDNeaV",
    "outputId": "02ea31cc-a3e8-4b9e-da02-1f2821ca4e52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test, batch_size=128,shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NVrCGU9jNkJb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (max_pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (i_downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (batch_norm3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = './resNet50_cifar_200_epochs.pth'\n",
    "net = ResNet50(10)\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "obf7QfWYOBFT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# net = ResNet50(10).to('cuda')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [64, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [64, 64, 16, 16]             128\n",
      "              ReLU-3           [64, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [64, 64, 8, 8]               0\n",
      "            Conv2d-5             [64, 64, 8, 8]           4,160\n",
      "       BatchNorm2d-6             [64, 64, 8, 8]             128\n",
      "              ReLU-7             [64, 64, 8, 8]               0\n",
      "            Conv2d-8             [64, 64, 8, 8]          36,928\n",
      "       BatchNorm2d-9             [64, 64, 8, 8]             128\n",
      "             ReLU-10             [64, 64, 8, 8]               0\n",
      "           Conv2d-11            [64, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-12            [64, 256, 8, 8]             512\n",
      "           Conv2d-13            [64, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-14            [64, 256, 8, 8]             512\n",
      "             ReLU-15            [64, 256, 8, 8]               0\n",
      "       Bottleneck-16            [64, 256, 8, 8]               0\n",
      "           Conv2d-17             [64, 64, 8, 8]          16,448\n",
      "      BatchNorm2d-18             [64, 64, 8, 8]             128\n",
      "             ReLU-19             [64, 64, 8, 8]               0\n",
      "           Conv2d-20             [64, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-21             [64, 64, 8, 8]             128\n",
      "             ReLU-22             [64, 64, 8, 8]               0\n",
      "           Conv2d-23            [64, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-24            [64, 256, 8, 8]             512\n",
      "             ReLU-25            [64, 256, 8, 8]               0\n",
      "       Bottleneck-26            [64, 256, 8, 8]               0\n",
      "           Conv2d-27             [64, 64, 8, 8]          16,448\n",
      "      BatchNorm2d-28             [64, 64, 8, 8]             128\n",
      "             ReLU-29             [64, 64, 8, 8]               0\n",
      "           Conv2d-30             [64, 64, 8, 8]          36,928\n",
      "      BatchNorm2d-31             [64, 64, 8, 8]             128\n",
      "             ReLU-32             [64, 64, 8, 8]               0\n",
      "           Conv2d-33            [64, 256, 8, 8]          16,640\n",
      "      BatchNorm2d-34            [64, 256, 8, 8]             512\n",
      "             ReLU-35            [64, 256, 8, 8]               0\n",
      "       Bottleneck-36            [64, 256, 8, 8]               0\n",
      "           Conv2d-37            [64, 128, 8, 8]          32,896\n",
      "      BatchNorm2d-38            [64, 128, 8, 8]             256\n",
      "             ReLU-39            [64, 128, 8, 8]               0\n",
      "           Conv2d-40            [64, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-41            [64, 128, 4, 4]             256\n",
      "             ReLU-42            [64, 128, 4, 4]               0\n",
      "           Conv2d-43            [64, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-44            [64, 512, 4, 4]           1,024\n",
      "           Conv2d-45            [64, 512, 4, 4]         131,584\n",
      "      BatchNorm2d-46            [64, 512, 4, 4]           1,024\n",
      "             ReLU-47            [64, 512, 4, 4]               0\n",
      "       Bottleneck-48            [64, 512, 4, 4]               0\n",
      "           Conv2d-49            [64, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-50            [64, 128, 4, 4]             256\n",
      "             ReLU-51            [64, 128, 4, 4]               0\n",
      "           Conv2d-52            [64, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-53            [64, 128, 4, 4]             256\n",
      "             ReLU-54            [64, 128, 4, 4]               0\n",
      "           Conv2d-55            [64, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-56            [64, 512, 4, 4]           1,024\n",
      "             ReLU-57            [64, 512, 4, 4]               0\n",
      "       Bottleneck-58            [64, 512, 4, 4]               0\n",
      "           Conv2d-59            [64, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-60            [64, 128, 4, 4]             256\n",
      "             ReLU-61            [64, 128, 4, 4]               0\n",
      "           Conv2d-62            [64, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-63            [64, 128, 4, 4]             256\n",
      "             ReLU-64            [64, 128, 4, 4]               0\n",
      "           Conv2d-65            [64, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-66            [64, 512, 4, 4]           1,024\n",
      "             ReLU-67            [64, 512, 4, 4]               0\n",
      "       Bottleneck-68            [64, 512, 4, 4]               0\n",
      "           Conv2d-69            [64, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-70            [64, 128, 4, 4]             256\n",
      "             ReLU-71            [64, 128, 4, 4]               0\n",
      "           Conv2d-72            [64, 128, 4, 4]         147,584\n",
      "      BatchNorm2d-73            [64, 128, 4, 4]             256\n",
      "             ReLU-74            [64, 128, 4, 4]               0\n",
      "           Conv2d-75            [64, 512, 4, 4]          66,048\n",
      "      BatchNorm2d-76            [64, 512, 4, 4]           1,024\n",
      "             ReLU-77            [64, 512, 4, 4]               0\n",
      "       Bottleneck-78            [64, 512, 4, 4]               0\n",
      "           Conv2d-79            [64, 256, 4, 4]         131,328\n",
      "      BatchNorm2d-80            [64, 256, 4, 4]             512\n",
      "             ReLU-81            [64, 256, 4, 4]               0\n",
      "           Conv2d-82            [64, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-83            [64, 256, 2, 2]             512\n",
      "             ReLU-84            [64, 256, 2, 2]               0\n",
      "           Conv2d-85           [64, 1024, 2, 2]         263,168\n",
      "      BatchNorm2d-86           [64, 1024, 2, 2]           2,048\n",
      "           Conv2d-87           [64, 1024, 2, 2]         525,312\n",
      "      BatchNorm2d-88           [64, 1024, 2, 2]           2,048\n",
      "             ReLU-89           [64, 1024, 2, 2]               0\n",
      "       Bottleneck-90           [64, 1024, 2, 2]               0\n",
      "           Conv2d-91            [64, 256, 2, 2]         262,400\n",
      "      BatchNorm2d-92            [64, 256, 2, 2]             512\n",
      "             ReLU-93            [64, 256, 2, 2]               0\n",
      "           Conv2d-94            [64, 256, 2, 2]         590,080\n",
      "      BatchNorm2d-95            [64, 256, 2, 2]             512\n",
      "             ReLU-96            [64, 256, 2, 2]               0\n",
      "           Conv2d-97           [64, 1024, 2, 2]         263,168\n",
      "      BatchNorm2d-98           [64, 1024, 2, 2]           2,048\n",
      "             ReLU-99           [64, 1024, 2, 2]               0\n",
      "      Bottleneck-100           [64, 1024, 2, 2]               0\n",
      "          Conv2d-101            [64, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-102            [64, 256, 2, 2]             512\n",
      "            ReLU-103            [64, 256, 2, 2]               0\n",
      "          Conv2d-104            [64, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-105            [64, 256, 2, 2]             512\n",
      "            ReLU-106            [64, 256, 2, 2]               0\n",
      "          Conv2d-107           [64, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-108           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-109           [64, 1024, 2, 2]               0\n",
      "      Bottleneck-110           [64, 1024, 2, 2]               0\n",
      "          Conv2d-111            [64, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-112            [64, 256, 2, 2]             512\n",
      "            ReLU-113            [64, 256, 2, 2]               0\n",
      "          Conv2d-114            [64, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-115            [64, 256, 2, 2]             512\n",
      "            ReLU-116            [64, 256, 2, 2]               0\n",
      "          Conv2d-117           [64, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-118           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-119           [64, 1024, 2, 2]               0\n",
      "      Bottleneck-120           [64, 1024, 2, 2]               0\n",
      "          Conv2d-121            [64, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-122            [64, 256, 2, 2]             512\n",
      "            ReLU-123            [64, 256, 2, 2]               0\n",
      "          Conv2d-124            [64, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-125            [64, 256, 2, 2]             512\n",
      "            ReLU-126            [64, 256, 2, 2]               0\n",
      "          Conv2d-127           [64, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-128           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-129           [64, 1024, 2, 2]               0\n",
      "      Bottleneck-130           [64, 1024, 2, 2]               0\n",
      "          Conv2d-131            [64, 256, 2, 2]         262,400\n",
      "     BatchNorm2d-132            [64, 256, 2, 2]             512\n",
      "            ReLU-133            [64, 256, 2, 2]               0\n",
      "          Conv2d-134            [64, 256, 2, 2]         590,080\n",
      "     BatchNorm2d-135            [64, 256, 2, 2]             512\n",
      "            ReLU-136            [64, 256, 2, 2]               0\n",
      "          Conv2d-137           [64, 1024, 2, 2]         263,168\n",
      "     BatchNorm2d-138           [64, 1024, 2, 2]           2,048\n",
      "            ReLU-139           [64, 1024, 2, 2]               0\n",
      "      Bottleneck-140           [64, 1024, 2, 2]               0\n",
      "          Conv2d-141            [64, 512, 2, 2]         524,800\n",
      "     BatchNorm2d-142            [64, 512, 2, 2]           1,024\n",
      "            ReLU-143            [64, 512, 2, 2]               0\n",
      "          Conv2d-144            [64, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-145            [64, 512, 1, 1]           1,024\n",
      "            ReLU-146            [64, 512, 1, 1]               0\n",
      "          Conv2d-147           [64, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-148           [64, 2048, 1, 1]           4,096\n",
      "          Conv2d-149           [64, 2048, 1, 1]       2,099,200\n",
      "     BatchNorm2d-150           [64, 2048, 1, 1]           4,096\n",
      "            ReLU-151           [64, 2048, 1, 1]               0\n",
      "      Bottleneck-152           [64, 2048, 1, 1]               0\n",
      "          Conv2d-153            [64, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-154            [64, 512, 1, 1]           1,024\n",
      "            ReLU-155            [64, 512, 1, 1]               0\n",
      "          Conv2d-156            [64, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-157            [64, 512, 1, 1]           1,024\n",
      "            ReLU-158            [64, 512, 1, 1]               0\n",
      "          Conv2d-159           [64, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-160           [64, 2048, 1, 1]           4,096\n",
      "            ReLU-161           [64, 2048, 1, 1]               0\n",
      "      Bottleneck-162           [64, 2048, 1, 1]               0\n",
      "          Conv2d-163            [64, 512, 1, 1]       1,049,088\n",
      "     BatchNorm2d-164            [64, 512, 1, 1]           1,024\n",
      "            ReLU-165            [64, 512, 1, 1]               0\n",
      "          Conv2d-166            [64, 512, 1, 1]       2,359,808\n",
      "     BatchNorm2d-167            [64, 512, 1, 1]           1,024\n",
      "            ReLU-168            [64, 512, 1, 1]               0\n",
      "          Conv2d-169           [64, 2048, 1, 1]       1,050,624\n",
      "     BatchNorm2d-170           [64, 2048, 1, 1]           4,096\n",
      "            ReLU-171           [64, 2048, 1, 1]               0\n",
      "      Bottleneck-172           [64, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-173           [64, 2048, 1, 1]               0\n",
      "          Linear-174                   [64, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,555,018\n",
      "Trainable params: 23,555,018\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 375.25\n",
      "Params size (MB): 89.86\n",
      "Estimated Total Size (MB): 465.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(net, (3,32, 32),64,device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Sp3I3vApPK2w",
    "outputId": "60d8be96-76cf-4ca8-8451-9ef862d1ef19",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [1, 100](epoch, minibatch):  8.586315660476684\n",
      "Loss [1, 200](epoch, minibatch):  2.9610777282714844\n",
      "Loss [1, 300](epoch, minibatch):  2.4179257237911225\n",
      "Loss [2, 100](epoch, minibatch):  2.049711730480194\n",
      "Loss [2, 200](epoch, minibatch):  1.9753414154052735\n",
      "Loss [2, 300](epoch, minibatch):  1.904314067363739\n",
      "Loss [3, 100](epoch, minibatch):  1.8436583340168\n",
      "Loss [3, 200](epoch, minibatch):  1.7955202376842498\n",
      "Loss [3, 300](epoch, minibatch):  1.7686460900306702\n",
      "Loss [4, 100](epoch, minibatch):  1.7251257956027986\n",
      "Loss [4, 200](epoch, minibatch):  1.6807338118553161\n",
      "Loss [4, 300](epoch, minibatch):  1.6840068590641022\n",
      "Loss [5, 100](epoch, minibatch):  1.637706241607666\n",
      "Loss [5, 200](epoch, minibatch):  1.606916286945343\n",
      "Loss [5, 300](epoch, minibatch):  1.567056783437729\n",
      "Loss [6, 100](epoch, minibatch):  1.5497723591327668\n",
      "Loss [6, 200](epoch, minibatch):  1.498753387928009\n",
      "Loss [6, 300](epoch, minibatch):  1.4916783726215364\n",
      "Loss [7, 100](epoch, minibatch):  1.4509439957141876\n",
      "Loss [7, 200](epoch, minibatch):  1.4280297482013702\n",
      "Loss [7, 300](epoch, minibatch):  1.4115448272228241\n",
      "Loss [8, 100](epoch, minibatch):  1.374373233318329\n",
      "Loss [8, 200](epoch, minibatch):  1.3623053205013276\n",
      "Loss [8, 300](epoch, minibatch):  1.3274951708316802\n",
      "Loss [9, 100](epoch, minibatch):  1.3012726056575774\n",
      "Loss [9, 200](epoch, minibatch):  1.305255845785141\n",
      "Loss [9, 300](epoch, minibatch):  1.262174940109253\n",
      "Loss [10, 100](epoch, minibatch):  1.254612430334091\n",
      "Loss [10, 200](epoch, minibatch):  1.231857381463051\n",
      "Loss [10, 300](epoch, minibatch):  1.232949492931366\n",
      "Loss [11, 100](epoch, minibatch):  1.1791856384277344\n",
      "Loss [11, 200](epoch, minibatch):  1.1878806638717652\n",
      "Loss [11, 300](epoch, minibatch):  1.1631704843044282\n",
      "Loss [12, 100](epoch, minibatch):  1.1549063324928284\n",
      "Loss [12, 200](epoch, minibatch):  1.1234562885761261\n",
      "Loss [12, 300](epoch, minibatch):  1.128008821606636\n",
      "Loss [13, 100](epoch, minibatch):  1.091439238190651\n",
      "Loss [13, 200](epoch, minibatch):  1.07847633600235\n",
      "Loss [13, 300](epoch, minibatch):  1.0686920601129533\n",
      "Loss [14, 100](epoch, minibatch):  1.0459600937366487\n",
      "Loss [14, 200](epoch, minibatch):  1.0270485186576843\n",
      "Loss [14, 300](epoch, minibatch):  1.0157036995887756\n",
      "Loss [15, 100](epoch, minibatch):  0.9958367675542832\n",
      "Loss [15, 200](epoch, minibatch):  0.9684120309352875\n",
      "Loss [15, 300](epoch, minibatch):  0.9664781069755555\n",
      "Loss [16, 100](epoch, minibatch):  0.9861952883005142\n",
      "Loss [16, 200](epoch, minibatch):  1.0061572468280793\n",
      "Loss [16, 300](epoch, minibatch):  0.9784321367740632\n",
      "Loss [17, 100](epoch, minibatch):  0.9449143397808075\n",
      "Loss [17, 200](epoch, minibatch):  0.987979844212532\n",
      "Loss [17, 300](epoch, minibatch):  1.0030383425951004\n",
      "Loss [18, 100](epoch, minibatch):  0.9381806939840317\n",
      "Loss [18, 200](epoch, minibatch):  0.9132867932319642\n",
      "Loss [18, 300](epoch, minibatch):  0.9438596045970917\n",
      "Loss [19, 100](epoch, minibatch):  0.9748215258121491\n",
      "Loss [19, 200](epoch, minibatch):  0.9074112373590469\n",
      "Loss [19, 300](epoch, minibatch):  0.8921791815757751\n",
      "Loss [20, 100](epoch, minibatch):  0.8502181851863861\n",
      "Loss [20, 200](epoch, minibatch):  0.8243424940109253\n",
      "Loss [20, 300](epoch, minibatch):  0.8325990736484528\n",
      "Loss [21, 100](epoch, minibatch):  0.8654883950948715\n",
      "Loss [21, 200](epoch, minibatch):  0.8156638926267624\n",
      "Loss [21, 300](epoch, minibatch):  0.8152906364202499\n",
      "Loss [22, 100](epoch, minibatch):  0.8102504563331604\n",
      "Loss [22, 200](epoch, minibatch):  0.7960645401477814\n",
      "Loss [22, 300](epoch, minibatch):  0.7765000492334366\n",
      "Loss [23, 100](epoch, minibatch):  0.8072313499450684\n",
      "Loss [23, 200](epoch, minibatch):  0.8048477947711945\n",
      "Loss [23, 300](epoch, minibatch):  0.8210642766952515\n",
      "Loss [24, 100](epoch, minibatch):  1.1870648050308228\n",
      "Loss [24, 200](epoch, minibatch):  1.7385624742507935\n",
      "Loss [24, 300](epoch, minibatch):  1.5641033589839934\n",
      "Loss [25, 100](epoch, minibatch):  1.3983531892299652\n",
      "Loss [25, 200](epoch, minibatch):  1.299202502965927\n",
      "Loss [25, 300](epoch, minibatch):  1.2618896359205245\n",
      "Loss [26, 100](epoch, minibatch):  1.1624207758903504\n",
      "Loss [26, 200](epoch, minibatch):  1.0855547291040422\n",
      "Loss [26, 300](epoch, minibatch):  1.0719702565670013\n",
      "Loss [27, 100](epoch, minibatch):  0.9995227307081223\n",
      "Loss [27, 300](epoch, minibatch):  0.9856043916940689\n",
      "Loss [28, 100](epoch, minibatch):  0.9053444337844848\n",
      "Loss [28, 200](epoch, minibatch):  0.9129476124048232\n",
      "Loss [28, 300](epoch, minibatch):  0.9014752757549286\n",
      "Loss [29, 100](epoch, minibatch):  0.8151075613498687\n",
      "Loss [29, 200](epoch, minibatch):  0.7653343743085861\n",
      "Loss [29, 300](epoch, minibatch):  0.7546287417411804\n",
      "Loss [30, 100](epoch, minibatch):  0.7305483192205429\n",
      "Loss [30, 200](epoch, minibatch):  0.726772700548172\n",
      "Loss [30, 300](epoch, minibatch):  0.7168963783979416\n",
      "Loss [31, 100](epoch, minibatch):  0.7246233388781548\n",
      "Loss [31, 200](epoch, minibatch):  0.710225378870964\n",
      "Loss [31, 300](epoch, minibatch):  0.7050363391637802\n",
      "Loss [32, 100](epoch, minibatch):  0.7099348676204681\n",
      "Loss [32, 200](epoch, minibatch):  0.6907766649127006\n",
      "Loss [32, 300](epoch, minibatch):  0.6874926722049713\n",
      "Loss [33, 100](epoch, minibatch):  0.6812802463769912\n",
      "Loss [33, 200](epoch, minibatch):  0.6778072020411492\n",
      "Loss [33, 300](epoch, minibatch):  0.690658755004406\n",
      "Loss [34, 100](epoch, minibatch):  0.6744991552829742\n",
      "Loss [34, 200](epoch, minibatch):  0.6690988275408745\n",
      "Loss [34, 300](epoch, minibatch):  0.6722329956293106\n",
      "Loss [35, 100](epoch, minibatch):  0.6691570526361466\n",
      "Loss [35, 200](epoch, minibatch):  0.6490276461839676\n",
      "Loss [35, 300](epoch, minibatch):  0.6731534457206726\n",
      "Loss [36, 100](epoch, minibatch):  0.6384114027023315\n",
      "Loss [36, 200](epoch, minibatch):  0.6642373323440551\n",
      "Loss [36, 300](epoch, minibatch):  0.6503935268521309\n",
      "Loss [37, 100](epoch, minibatch):  0.6452614814043045\n",
      "Loss [37, 200](epoch, minibatch):  0.6561760640144348\n",
      "Loss [37, 300](epoch, minibatch):  0.6230297514796257\n",
      "Loss [38, 100](epoch, minibatch):  0.6331321361660958\n",
      "Loss [38, 200](epoch, minibatch):  0.6296942752599716\n",
      "Loss [38, 300](epoch, minibatch):  0.6151263493299485\n",
      "Loss [39, 100](epoch, minibatch):  0.6401380595564842\n",
      "Loss [39, 200](epoch, minibatch):  0.6163411435484886\n",
      "Loss [39, 300](epoch, minibatch):  0.605560839176178\n",
      "Loss [40, 100](epoch, minibatch):  0.6268535861372948\n",
      "Loss [40, 200](epoch, minibatch):  0.6077457910776138\n",
      "Loss [40, 300](epoch, minibatch):  0.6235954982042312\n",
      "Loss [41, 100](epoch, minibatch):  0.6062695407867431\n",
      "Loss [41, 200](epoch, minibatch):  0.6063808453083038\n",
      "Loss [41, 300](epoch, minibatch):  0.6080471065640449\n",
      "Loss [42, 100](epoch, minibatch):  0.5982637998461723\n",
      "Loss [42, 200](epoch, minibatch):  0.601314609348774\n",
      "Loss [42, 300](epoch, minibatch):  0.5965665364265442\n",
      "Loss [43, 100](epoch, minibatch):  0.6051661339402199\n",
      "Loss [43, 200](epoch, minibatch):  0.6085859015583992\n",
      "Loss [43, 300](epoch, minibatch):  0.5942464464902878\n",
      "Loss [44, 100](epoch, minibatch):  0.5897631099820138\n",
      "Loss [44, 200](epoch, minibatch):  0.584155875146389\n",
      "Loss [44, 300](epoch, minibatch):  0.5921912124752998\n",
      "Loss [45, 100](epoch, minibatch):  0.5839641797542572\n",
      "Loss [45, 200](epoch, minibatch):  0.5854407131671906\n",
      "Loss [45, 300](epoch, minibatch):  0.5784676158428192\n",
      "Loss [46, 100](epoch, minibatch):  0.5675587323307991\n",
      "Loss [46, 200](epoch, minibatch):  0.5629541158676148\n",
      "Loss [46, 300](epoch, minibatch):  0.5720343801379204\n",
      "Loss [47, 100](epoch, minibatch):  0.5644165405631065\n",
      "Loss [47, 200](epoch, minibatch):  0.5557251444458962\n",
      "Loss [47, 300](epoch, minibatch):  0.5608779427409172\n",
      "Loss [48, 100](epoch, minibatch):  0.5522192916274071\n",
      "Loss [48, 200](epoch, minibatch):  0.5524486020207405\n",
      "Loss [48, 300](epoch, minibatch):  0.5576543352007866\n",
      "Loss [49, 100](epoch, minibatch):  0.550587278008461\n",
      "Loss [49, 200](epoch, minibatch):  0.5506322261691093\n",
      "Loss [49, 300](epoch, minibatch):  0.5525056791305542\n",
      "Loss [50, 100](epoch, minibatch):  0.5361247950792313\n",
      "Loss [50, 200](epoch, minibatch):  0.5554055500030518\n",
      "Loss [50, 300](epoch, minibatch):  0.5409849029779434\n",
      "Loss [51, 100](epoch, minibatch):  0.5273888182640075\n",
      "Loss [51, 200](epoch, minibatch):  0.5445154300332069\n",
      "Loss [51, 300](epoch, minibatch):  0.5265077498555183\n",
      "Loss [52, 100](epoch, minibatch):  0.5191629785299301\n",
      "Loss [52, 200](epoch, minibatch):  0.5148633721470833\n",
      "Loss [52, 300](epoch, minibatch):  0.5494846886396408\n",
      "Loss [53, 100](epoch, minibatch):  0.5257783988118172\n",
      "Loss [53, 200](epoch, minibatch):  0.5088289731740951\n",
      "Loss [53, 300](epoch, minibatch):  0.5211255687475205\n",
      "Loss [54, 100](epoch, minibatch):  0.5196862679719925\n",
      "Loss [54, 200](epoch, minibatch):  0.5048244768381118\n",
      "Loss [54, 300](epoch, minibatch):  0.5179332235455513\n",
      "Loss [55, 100](epoch, minibatch):  0.523520736694336\n",
      "Loss [55, 200](epoch, minibatch):  0.5057293337583542\n",
      "Loss [55, 300](epoch, minibatch):  0.4996830576658249\n",
      "Loss [56, 100](epoch, minibatch):  0.5078681150078773\n",
      "Loss [56, 200](epoch, minibatch):  0.49900413751602174\n",
      "Loss [56, 300](epoch, minibatch):  0.5029421359300613\n",
      "Loss [57, 100](epoch, minibatch):  0.5020668143033982\n",
      "Loss [57, 200](epoch, minibatch):  0.4901755908131599\n",
      "Loss [57, 300](epoch, minibatch):  0.4890377399325371\n",
      "Loss [58, 100](epoch, minibatch):  0.49157766371965406\n",
      "Loss [58, 200](epoch, minibatch):  0.4824986729025841\n",
      "Loss [58, 300](epoch, minibatch):  0.48973281413316727\n",
      "Loss [59, 100](epoch, minibatch):  0.49092262864112857\n",
      "Loss [59, 200](epoch, minibatch):  0.4886430296301842\n",
      "Loss [59, 300](epoch, minibatch):  0.4972332063317299\n",
      "Loss [60, 100](epoch, minibatch):  0.4798957014083862\n",
      "Loss [60, 200](epoch, minibatch):  0.46687218844890593\n",
      "Loss [60, 300](epoch, minibatch):  0.48200520366430283\n",
      "Loss [61, 100](epoch, minibatch):  0.46598499834537505\n",
      "Loss [61, 200](epoch, minibatch):  0.4745635038614273\n",
      "Loss [61, 300](epoch, minibatch):  0.47816752940416335\n",
      "Loss [62, 100](epoch, minibatch):  0.4669114649295807\n",
      "Loss [62, 200](epoch, minibatch):  0.4611586380004883\n",
      "Loss [62, 300](epoch, minibatch):  0.47628203898668287\n",
      "Loss [63, 100](epoch, minibatch):  0.4565191841125488\n",
      "Loss [63, 200](epoch, minibatch):  0.4669097951054573\n",
      "Loss [63, 300](epoch, minibatch):  0.4680137059092522\n",
      "Loss [64, 100](epoch, minibatch):  0.4549554771184921\n",
      "Loss [64, 200](epoch, minibatch):  0.44362938970327376\n",
      "Loss [64, 300](epoch, minibatch):  0.4751603099703789\n",
      "Loss [65, 100](epoch, minibatch):  0.43872921526432035\n",
      "Loss [65, 200](epoch, minibatch):  0.45190739959478377\n",
      "Loss [65, 300](epoch, minibatch):  0.45486554443836213\n",
      "Loss [66, 100](epoch, minibatch):  0.4553199568390846\n",
      "Loss [66, 200](epoch, minibatch):  0.4409274932742119\n",
      "Loss [66, 300](epoch, minibatch):  0.453617924451828\n",
      "Loss [67, 100](epoch, minibatch):  0.4509198480844498\n",
      "Loss [67, 200](epoch, minibatch):  0.44471113204956053\n",
      "Loss [67, 300](epoch, minibatch):  0.4387761425971985\n",
      "Loss [68, 100](epoch, minibatch):  0.43845456749200823\n",
      "Loss [68, 200](epoch, minibatch):  0.43107393980026243\n",
      "Loss [68, 300](epoch, minibatch):  0.43281990587711333\n",
      "Loss [69, 100](epoch, minibatch):  0.4398021250963211\n",
      "Loss [69, 200](epoch, minibatch):  0.43386766612529754\n",
      "Loss [69, 300](epoch, minibatch):  0.43097212821245195\n",
      "Loss [70, 100](epoch, minibatch):  0.4196459473669529\n",
      "Loss [70, 200](epoch, minibatch):  0.43176748245954516\n",
      "Loss [70, 300](epoch, minibatch):  0.43073197215795517\n",
      "Loss [71, 100](epoch, minibatch):  0.4050948059558868\n",
      "Loss [71, 200](epoch, minibatch):  0.4374657228589058\n",
      "Loss [71, 300](epoch, minibatch):  0.42112499102950096\n",
      "Loss [72, 100](epoch, minibatch):  0.41735900327563286\n",
      "Loss [72, 200](epoch, minibatch):  0.41438709050416944\n",
      "Loss [72, 300](epoch, minibatch):  0.41882645815610886\n",
      "Loss [73, 100](epoch, minibatch):  0.4243933914601803\n",
      "Loss [73, 200](epoch, minibatch):  0.40899084478616715\n",
      "Loss [73, 300](epoch, minibatch):  0.41148725599050523\n",
      "Loss [74, 100](epoch, minibatch):  0.4027266997098923\n",
      "Loss [74, 200](epoch, minibatch):  0.4093684020638466\n",
      "Loss [74, 300](epoch, minibatch):  0.4103274914622307\n",
      "Loss [75, 100](epoch, minibatch):  0.40071594148874284\n",
      "Loss [75, 200](epoch, minibatch):  0.3833668705821037\n",
      "Loss [75, 300](epoch, minibatch):  0.39322510197758676\n",
      "Loss [76, 100](epoch, minibatch):  0.39568376749753953\n",
      "Loss [76, 200](epoch, minibatch):  0.39145960986614226\n",
      "Loss [76, 300](epoch, minibatch):  0.4020399333536625\n",
      "Loss [77, 100](epoch, minibatch):  0.3968863387405872\n",
      "Loss [77, 200](epoch, minibatch):  0.39985021024942397\n",
      "Loss [77, 300](epoch, minibatch):  0.39179876536130903\n",
      "Loss [78, 100](epoch, minibatch):  0.38786730483174325\n",
      "Loss [78, 200](epoch, minibatch):  0.3920197941362858\n",
      "Loss [78, 300](epoch, minibatch):  0.3961332169175148\n",
      "Loss [79, 100](epoch, minibatch):  0.3896317492425442\n",
      "Loss [79, 200](epoch, minibatch):  0.38027730882167815\n",
      "Loss [79, 300](epoch, minibatch):  0.3893929794430733\n",
      "Loss [80, 100](epoch, minibatch):  0.39263032883405685\n",
      "Loss [80, 200](epoch, minibatch):  0.3747378472983837\n",
      "Loss [80, 300](epoch, minibatch):  0.38062964990735054\n",
      "Loss [81, 100](epoch, minibatch):  0.36614442050457\n",
      "Loss [81, 200](epoch, minibatch):  0.3768167470395565\n",
      "Loss [81, 300](epoch, minibatch):  0.39214538380503655\n",
      "Loss [82, 100](epoch, minibatch):  0.37806264132261275\n",
      "Loss [82, 200](epoch, minibatch):  0.36992390021681787\n",
      "Loss [82, 300](epoch, minibatch):  0.38284859627485274\n",
      "Loss [83, 100](epoch, minibatch):  0.3626218402385712\n",
      "Loss [83, 200](epoch, minibatch):  0.36809886157512667\n",
      "Loss [83, 300](epoch, minibatch):  0.37031109184026717\n",
      "Loss [84, 100](epoch, minibatch):  0.3608327469229698\n",
      "Loss [84, 200](epoch, minibatch):  0.3743697364628315\n",
      "Loss [84, 300](epoch, minibatch):  0.37023984745144844\n",
      "Loss [85, 100](epoch, minibatch):  0.3487425076961517\n",
      "Loss [85, 200](epoch, minibatch):  0.3616396138072014\n",
      "Loss [85, 300](epoch, minibatch):  0.3661057344079018\n",
      "Loss [86, 100](epoch, minibatch):  0.3589955015480518\n",
      "Loss [86, 200](epoch, minibatch):  0.3651145181059837\n",
      "Loss [86, 300](epoch, minibatch):  0.3611297391355038\n",
      "Loss [87, 100](epoch, minibatch):  0.3464437398314476\n",
      "Loss [87, 200](epoch, minibatch):  0.3570671519637108\n",
      "Loss [87, 300](epoch, minibatch):  0.3534110215306282\n",
      "Loss [88, 100](epoch, minibatch):  0.3436980001628399\n",
      "Loss [88, 200](epoch, minibatch):  0.3668970164656639\n",
      "Loss [88, 300](epoch, minibatch):  0.3545545676350594\n",
      "Loss [89, 100](epoch, minibatch):  0.3377390570938587\n",
      "Loss [89, 200](epoch, minibatch):  0.35090104609727857\n",
      "Loss [89, 300](epoch, minibatch):  0.35138614699244497\n",
      "Loss [90, 100](epoch, minibatch):  0.33993664413690566\n",
      "Loss [90, 200](epoch, minibatch):  0.34169562593102454\n",
      "Loss [90, 300](epoch, minibatch):  0.34600990772247314\n",
      "Loss [91, 100](epoch, minibatch):  0.33018734693527224\n",
      "Loss [91, 200](epoch, minibatch):  0.34724631503224374\n",
      "Loss [91, 300](epoch, minibatch):  0.3367656373977661\n",
      "Loss [92, 100](epoch, minibatch):  0.3226696985960007\n",
      "Loss [92, 200](epoch, minibatch):  0.34000035911798476\n",
      "Loss [92, 300](epoch, minibatch):  0.3346186526119709\n",
      "Loss [93, 100](epoch, minibatch):  0.3309314176440239\n",
      "Loss [93, 200](epoch, minibatch):  0.3290463604032993\n",
      "Loss [93, 300](epoch, minibatch):  0.33743887290358543\n",
      "Loss [94, 100](epoch, minibatch):  0.3239282570779324\n",
      "Loss [94, 200](epoch, minibatch):  0.3362798929214478\n",
      "Loss [94, 300](epoch, minibatch):  0.33869170531630516\n",
      "Loss [95, 100](epoch, minibatch):  0.3168335688114166\n",
      "Loss [95, 200](epoch, minibatch):  0.3204029043018818\n",
      "Loss [95, 300](epoch, minibatch):  0.3291657921671867\n",
      "Loss [96, 100](epoch, minibatch):  0.31142474845051765\n",
      "Loss [96, 200](epoch, minibatch):  0.32647439584136007\n",
      "Loss [96, 300](epoch, minibatch):  0.33610051319003104\n",
      "Loss [97, 100](epoch, minibatch):  0.3061800867319107\n",
      "Loss [97, 200](epoch, minibatch):  0.3129424099624157\n",
      "Loss [97, 300](epoch, minibatch):  0.3315944801270962\n",
      "Loss [98, 100](epoch, minibatch):  0.30857987478375437\n",
      "Loss [98, 200](epoch, minibatch):  0.3160925705730915\n",
      "Loss [98, 300](epoch, minibatch):  0.32547559440135954\n",
      "Loss [99, 100](epoch, minibatch):  0.3050544162094593\n",
      "Loss [99, 200](epoch, minibatch):  0.3087383884191513\n",
      "Loss [99, 300](epoch, minibatch):  0.3208591331541538\n",
      "Loss [100, 100](epoch, minibatch):  0.29744609430432317\n",
      "Loss [100, 200](epoch, minibatch):  0.3146725448966026\n",
      "Loss [100, 300](epoch, minibatch):  0.3154247848689556\n",
      "Loss [101, 100](epoch, minibatch):  0.29752662107348443\n",
      "Loss [101, 200](epoch, minibatch):  0.3155659684538841\n",
      "Loss [101, 300](epoch, minibatch):  0.3104909572005272\n",
      "Loss [102, 100](epoch, minibatch):  0.30537095680832865\n",
      "Loss [102, 200](epoch, minibatch):  0.2943628738820553\n",
      "Loss [102, 300](epoch, minibatch):  0.30232407376170156\n",
      "Loss [103, 100](epoch, minibatch):  0.28770591646432875\n",
      "Loss [103, 200](epoch, minibatch):  0.2918100465834141\n",
      "Loss [103, 300](epoch, minibatch):  0.30244294196367266\n",
      "Loss [104, 100](epoch, minibatch):  0.28579834192991255\n",
      "Loss [104, 200](epoch, minibatch):  0.29977103844285014\n",
      "Loss [104, 300](epoch, minibatch):  0.29382980361580846\n",
      "Loss [105, 100](epoch, minibatch):  0.29596246406435966\n",
      "Loss [105, 200](epoch, minibatch):  0.3038742671906948\n",
      "Loss [105, 300](epoch, minibatch):  0.3044767914712429\n",
      "Loss [106, 100](epoch, minibatch):  0.2892045420408249\n",
      "Loss [106, 200](epoch, minibatch):  0.28055634304881094\n",
      "Loss [106, 300](epoch, minibatch):  0.29577186405658723\n",
      "Loss [107, 100](epoch, minibatch):  0.2784735582768917\n",
      "Loss [107, 200](epoch, minibatch):  0.2829146067798138\n",
      "Loss [107, 300](epoch, minibatch):  0.281219522356987\n",
      "Loss [108, 100](epoch, minibatch):  0.2762538665533066\n",
      "Loss [108, 200](epoch, minibatch):  0.2781443451344967\n",
      "Loss [108, 300](epoch, minibatch):  0.2940319490432739\n",
      "Loss [109, 100](epoch, minibatch):  0.26370065823197364\n",
      "Loss [109, 200](epoch, minibatch):  0.28298199534416196\n",
      "Loss [109, 300](epoch, minibatch):  0.2828238867223263\n",
      "Loss [110, 100](epoch, minibatch):  0.2752816845476627\n",
      "Loss [110, 200](epoch, minibatch):  0.2639979614317417\n",
      "Loss [110, 300](epoch, minibatch):  0.2859893243014813\n",
      "Loss [111, 100](epoch, minibatch):  0.262975649908185\n",
      "Loss [111, 200](epoch, minibatch):  0.26940063774585726\n",
      "Loss [111, 300](epoch, minibatch):  0.27556748256087304\n",
      "Loss [112, 100](epoch, minibatch):  0.24934540539979935\n",
      "Loss [112, 200](epoch, minibatch):  0.27764265075325967\n",
      "Loss [112, 300](epoch, minibatch):  0.2698668183386326\n",
      "Loss [113, 100](epoch, minibatch):  0.2524707394838333\n",
      "Loss [113, 200](epoch, minibatch):  0.27256586760282514\n",
      "Loss [113, 300](epoch, minibatch):  0.27199150919914244\n",
      "Loss [114, 100](epoch, minibatch):  0.26466383039951324\n",
      "Loss [114, 200](epoch, minibatch):  0.25396428868174553\n",
      "Loss [114, 300](epoch, minibatch):  0.27560869708657265\n",
      "Loss [115, 100](epoch, minibatch):  0.24495961561799048\n",
      "Loss [115, 200](epoch, minibatch):  0.2513961769640446\n",
      "Loss [115, 300](epoch, minibatch):  0.27344669818878176\n",
      "Loss [116, 100](epoch, minibatch):  0.2568753032386303\n",
      "Loss [116, 200](epoch, minibatch):  0.259434447735548\n",
      "Loss [116, 300](epoch, minibatch):  0.25510598309338095\n",
      "Loss [117, 100](epoch, minibatch):  0.25377280339598657\n",
      "Loss [117, 200](epoch, minibatch):  0.26102623477578163\n",
      "Loss [117, 300](epoch, minibatch):  0.257701705545187\n",
      "Loss [118, 100](epoch, minibatch):  0.2538676753640175\n",
      "Loss [118, 200](epoch, minibatch):  0.24535980224609374\n",
      "Loss [118, 300](epoch, minibatch):  0.2539057518541813\n",
      "Loss [119, 100](epoch, minibatch):  0.23840583726763726\n",
      "Loss [119, 200](epoch, minibatch):  0.24044742688536644\n",
      "Loss [119, 300](epoch, minibatch):  0.2548069839179516\n",
      "Loss [120, 100](epoch, minibatch):  0.23853958174586296\n",
      "Loss [120, 200](epoch, minibatch):  0.24504963487386702\n",
      "Loss [120, 300](epoch, minibatch):  0.25365946248173715\n",
      "Loss [121, 100](epoch, minibatch):  0.24629301607608794\n",
      "Loss [121, 200](epoch, minibatch):  0.24574870809912683\n",
      "Loss [121, 300](epoch, minibatch):  0.25492962390184404\n",
      "Loss [122, 100](epoch, minibatch):  0.24339060872793197\n",
      "Loss [122, 200](epoch, minibatch):  0.2474590538442135\n",
      "Loss [122, 300](epoch, minibatch):  0.24662789568305016\n",
      "Loss [123, 100](epoch, minibatch):  0.2324557613581419\n",
      "Loss [123, 200](epoch, minibatch):  0.24529382780194284\n",
      "Loss [123, 300](epoch, minibatch):  0.2505691701173782\n",
      "Loss [124, 100](epoch, minibatch):  0.22448870703577994\n",
      "Loss [124, 200](epoch, minibatch):  0.2392571684718132\n",
      "Loss [124, 300](epoch, minibatch):  0.234042659252882\n",
      "Loss [125, 100](epoch, minibatch):  0.22557126224040985\n",
      "Loss [125, 200](epoch, minibatch):  0.23677434638142586\n",
      "Loss [125, 300](epoch, minibatch):  0.24379406601190567\n",
      "Loss [126, 100](epoch, minibatch):  0.23807650208473205\n",
      "Loss [126, 200](epoch, minibatch):  0.22955102972686292\n",
      "Loss [126, 300](epoch, minibatch):  0.23265873044729232\n",
      "Loss [127, 100](epoch, minibatch):  0.22724107399582863\n",
      "Loss [127, 200](epoch, minibatch):  0.23513522744178772\n",
      "Loss [127, 300](epoch, minibatch):  0.23505494497716428\n",
      "Loss [128, 100](epoch, minibatch):  0.22893542557954788\n",
      "Loss [128, 200](epoch, minibatch):  0.2378268899023533\n",
      "Loss [128, 300](epoch, minibatch):  0.2339015406370163\n",
      "Loss [129, 100](epoch, minibatch):  0.21973496720194816\n",
      "Loss [129, 200](epoch, minibatch):  0.2250766619294882\n",
      "Loss [129, 300](epoch, minibatch):  0.23797354832291603\n",
      "Loss [130, 100](epoch, minibatch):  0.21225159376859665\n",
      "Loss [130, 200](epoch, minibatch):  0.22720492735505105\n",
      "Loss [130, 300](epoch, minibatch):  0.22856218971312045\n",
      "Loss [131, 100](epoch, minibatch):  0.21960947632789612\n",
      "Loss [131, 200](epoch, minibatch):  0.22285083636641503\n",
      "Loss [131, 300](epoch, minibatch):  0.21641657181084156\n",
      "Loss [132, 100](epoch, minibatch):  0.20700969062745572\n",
      "Loss [132, 200](epoch, minibatch):  0.2371260655671358\n",
      "Loss [132, 300](epoch, minibatch):  0.22261720955371855\n",
      "Loss [133, 100](epoch, minibatch):  0.21409260213375092\n",
      "Loss [133, 200](epoch, minibatch):  0.21182009994983672\n",
      "Loss [133, 300](epoch, minibatch):  0.22467197984457016\n",
      "Loss [134, 100](epoch, minibatch):  0.2141597158461809\n",
      "Loss [134, 200](epoch, minibatch):  0.21169372864067554\n",
      "Loss [134, 300](epoch, minibatch):  0.23429796531796454\n",
      "Loss [135, 100](epoch, minibatch):  0.21295611873269082\n",
      "Loss [135, 200](epoch, minibatch):  0.22779718697071075\n",
      "Loss [135, 300](epoch, minibatch):  0.21089092917740346\n",
      "Loss [136, 100](epoch, minibatch):  0.21089522562921048\n",
      "Loss [136, 200](epoch, minibatch):  0.21070289738476278\n",
      "Loss [136, 300](epoch, minibatch):  0.21506339341402053\n",
      "Loss [137, 100](epoch, minibatch):  0.20917560681700706\n",
      "Loss [137, 200](epoch, minibatch):  0.2096170287579298\n",
      "Loss [137, 300](epoch, minibatch):  0.21610671058297157\n",
      "Loss [138, 100](epoch, minibatch):  0.20111633844673635\n",
      "Loss [138, 200](epoch, minibatch):  0.2132163068652153\n",
      "Loss [138, 300](epoch, minibatch):  0.21457420997321605\n",
      "Loss [139, 100](epoch, minibatch):  0.20953606776893138\n",
      "Loss [139, 200](epoch, minibatch):  0.20674663446843625\n",
      "Loss [139, 300](epoch, minibatch):  0.20265406742691994\n",
      "Loss [140, 100](epoch, minibatch):  0.19880079366266729\n",
      "Loss [140, 200](epoch, minibatch):  0.2098150036484003\n",
      "Loss [140, 300](epoch, minibatch):  0.20655526049435138\n",
      "Loss [141, 100](epoch, minibatch):  0.18516048178076744\n",
      "Loss [141, 200](epoch, minibatch):  0.1951662427932024\n",
      "Loss [141, 300](epoch, minibatch):  0.20383672535419464\n",
      "Loss [142, 100](epoch, minibatch):  0.19138553395867347\n",
      "Loss [142, 200](epoch, minibatch):  0.2011611784249544\n",
      "Loss [142, 300](epoch, minibatch):  0.20427704334259034\n",
      "Loss [143, 100](epoch, minibatch):  0.18826145447790624\n",
      "Loss [143, 200](epoch, minibatch):  0.20768463291227818\n",
      "Loss [143, 300](epoch, minibatch):  0.19855123572051525\n",
      "Loss [144, 100](epoch, minibatch):  0.18721951939165593\n",
      "Loss [144, 200](epoch, minibatch):  0.19720082521438598\n",
      "Loss [144, 300](epoch, minibatch):  0.19991207763552665\n",
      "Loss [145, 100](epoch, minibatch):  0.1867906726151705\n",
      "Loss [145, 200](epoch, minibatch):  0.19432949468493463\n",
      "Loss [145, 300](epoch, minibatch):  0.2118118654191494\n",
      "Loss [146, 100](epoch, minibatch):  0.18659565158188343\n",
      "Loss [146, 200](epoch, minibatch):  0.1923379519581795\n",
      "Loss [146, 300](epoch, minibatch):  0.20931396663188934\n",
      "Loss [147, 100](epoch, minibatch):  0.17244940087199212\n",
      "Loss [147, 200](epoch, minibatch):  0.1861745848506689\n",
      "Loss [147, 300](epoch, minibatch):  0.1951798328012228\n",
      "Loss [148, 100](epoch, minibatch):  0.18890644535422324\n",
      "Loss [148, 200](epoch, minibatch):  0.19214738599956036\n",
      "Loss [148, 300](epoch, minibatch):  0.19064394235610962\n",
      "Loss [149, 100](epoch, minibatch):  0.18661906398832798\n",
      "Loss [149, 200](epoch, minibatch):  0.18185740515589713\n",
      "Loss [149, 300](epoch, minibatch):  0.1899231094866991\n",
      "Loss [150, 100](epoch, minibatch):  0.18192920945584773\n",
      "Loss [150, 200](epoch, minibatch):  0.18736015252768992\n",
      "Loss [150, 300](epoch, minibatch):  0.19157533682882785\n",
      "Loss [151, 100](epoch, minibatch):  0.1866135784238577\n",
      "Loss [151, 200](epoch, minibatch):  0.1893862175196409\n",
      "Loss [151, 300](epoch, minibatch):  0.17838380485773087\n",
      "Loss [152, 100](epoch, minibatch):  0.17203001618385316\n",
      "Loss [152, 200](epoch, minibatch):  0.18450678918510677\n",
      "Loss [152, 300](epoch, minibatch):  0.17514961533248424\n",
      "Loss [153, 100](epoch, minibatch):  0.17487573258578779\n",
      "Loss [153, 200](epoch, minibatch):  0.1761145942658186\n",
      "Loss [153, 300](epoch, minibatch):  0.1846049975603819\n",
      "Loss [154, 100](epoch, minibatch):  0.16527133055031298\n",
      "Loss [154, 200](epoch, minibatch):  0.1763790349662304\n",
      "Loss [154, 300](epoch, minibatch):  0.18320819981396197\n",
      "Loss [155, 100](epoch, minibatch):  0.17059913270175456\n",
      "Loss [155, 200](epoch, minibatch):  0.1797381604462862\n",
      "Loss [155, 300](epoch, minibatch):  0.18548951894044877\n",
      "Loss [156, 100](epoch, minibatch):  0.1687752613425255\n",
      "Loss [156, 200](epoch, minibatch):  0.1780370855331421\n",
      "Loss [156, 300](epoch, minibatch):  0.18115624397993088\n",
      "Loss [157, 100](epoch, minibatch):  0.1763369008898735\n",
      "Loss [157, 200](epoch, minibatch):  0.17140597626566886\n",
      "Loss [157, 300](epoch, minibatch):  0.1781178370118141\n",
      "Loss [158, 100](epoch, minibatch):  0.17280964471399785\n",
      "Loss [158, 200](epoch, minibatch):  0.17268616266548634\n",
      "Loss [158, 300](epoch, minibatch):  0.17383879616856576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     12\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AI course\\Deep Learning Projects\\CNN\\ResNet.py:99\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m     98\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m---> 99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m    102\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AI course\\Deep Learning Projects\\CNN\\ResNet.py:26\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     25\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    for i, inp in enumerate(trainloader):\n",
    "        inputs, labels = inp\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i%100 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}, {i}](epoch, minibatch): ', running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "            \n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH = './resNet50_cifar.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eY3rPw7wPVEe",
    "outputId": "213b5a1c-d9f4-4659-f6dc-b6ff9d0d1a44",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 10,000 test images:  83.88 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy on 10,000 test images: ', 100*(correct/total), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeVRGN56hkNZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CIFAR10-ResNet50_85%.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
